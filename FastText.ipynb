{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c5f16a",
   "metadata": {},
   "source": [
    "# FastText Deep NLP \n",
    "\n",
    "Using FastText library to create the best model for sentiment prediction on imdb dataset.\n",
    "\n",
    "#### Authors\n",
    "* Denjoy Segolene\n",
    "* Le Helloco Quentin\n",
    "* Sharpin Etienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1cc9c",
   "metadata": {},
   "source": [
    "### Import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "476a36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ebaf6",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "We use imdb huggingface dataset. It is composed of 25000 samples of positive/negative labeled review for training and 25000 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d25d5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/quentinlehelloco/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eecd060e3b456d813f0a114a6f93b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c56fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset[\"train\"][:][\"text\"]\n",
    "y_train = dataset[\"train\"][:][\"label\"]\n",
    "\n",
    "x_test = dataset[\"test\"][:][\"text\"]\n",
    "y_test = dataset[\"test\"][:][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fd117",
   "metadata": {},
   "source": [
    "Let's look at the first sample to see what kind of format is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe45f5ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7b1f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c829666",
   "metadata": {},
   "source": [
    "Samples are made of a single string composed of multiples sentences. The label indicates 1 for positives reviews and 0 for negatives ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced7173",
   "metadata": {},
   "source": [
    "### Format dataset for FastText usage\n",
    "\n",
    "FastText use a specific format of input: files where each line is a sample with **\\_\\_label\\_\\_\\<label_name\\>** before each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "46111e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4715e1df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Value  Positive\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...         1\n",
       "1  Homelessness (or Houselessness as George Carli...         1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...         1\n",
       "3  This is easily the most underrated film inn th...         1\n",
       "4  This is not the typical Mel Brooks film. It wa...         1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Value':x_train, 'Positive':y_train})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941a560",
   "metadata": {},
   "source": [
    "#### We need to add \"\\__label\\__positive\" and \"\\__label\\__negative\" before every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e6611d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(s:str, p:int) -> str:\n",
    "    if p == 1:\n",
    "        return \"__label__positive \" + s\n",
    "    else:\n",
    "        return \"__label__negative \" + s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3bcad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Value\"] = df.apply(lambda x: add_label(x.Value, x.Positive), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fcd55453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__positive Bromwell High is a cartoon c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__positive Homelessness (or Houselessne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__positive Brilliant over-acting by Les...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__positive This is easily the most unde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__positive This is not the typical Mel ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>__label__negative Towards the end of the movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>__label__negative This is the kind of movie th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>__label__negative I saw 'Descent' last night a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>__label__negative Some films that you pick up ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>__label__negative This is one of the dumbest f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Value  Positive\n",
       "0      __label__positive Bromwell High is a cartoon c...         1\n",
       "1      __label__positive Homelessness (or Houselessne...         1\n",
       "2      __label__positive Brilliant over-acting by Les...         1\n",
       "3      __label__positive This is easily the most unde...         1\n",
       "4      __label__positive This is not the typical Mel ...         1\n",
       "...                                                  ...       ...\n",
       "24995  __label__negative Towards the end of the movie...         0\n",
       "24996  __label__negative This is the kind of movie th...         0\n",
       "24997  __label__negative I saw 'Descent' last night a...         0\n",
       "24998  __label__negative Some films that you pick up ...         0\n",
       "24999  __label__negative This is one of the dumbest f...         0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0cc198",
   "metadata": {},
   "source": [
    "The samples are now nicely formatted to be put in a FastText input file. But because we do not want a bias by giving the model all positive samples first, we need to shuffle the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289c848",
   "metadata": {},
   "source": [
    "### Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4aa55aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__positive This is simply a classic fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__negative Wow. Some movies just leave ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__negative As an ex-teacher(!) I must c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__positive Robert Jannuci,Luca Venantin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__positive I would never have thought I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>__label__positive This project was originally ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>__label__positive When I first watched this, w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>__label__positive Madhur Bhandarkar goes all o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>__label__negative I rented this thinking it mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>__label__positive When I heard that Adrian Pas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Value  Positive\n",
       "0      __label__positive This is simply a classic fil...         1\n",
       "1      __label__negative Wow. Some movies just leave ...         0\n",
       "2      __label__negative As an ex-teacher(!) I must c...         0\n",
       "3      __label__positive Robert Jannuci,Luca Venantin...         1\n",
       "4      __label__positive I would never have thought I...         1\n",
       "...                                                  ...       ...\n",
       "24995  __label__positive This project was originally ...         1\n",
       "24996  __label__positive When I first watched this, w...         1\n",
       "24997  __label__positive Madhur Bhandarkar goes all o...         1\n",
       "24998  __label__negative I rented this thinking it mi...         0\n",
       "24999  __label__positive When I heard that Adrian Pas...         1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86953fad",
   "metadata": {},
   "source": [
    "The samples can now be put in a FasText input file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758beb4",
   "metadata": {},
   "source": [
    "### Put data as a file for FastText input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9478c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"FastText_input.txt\", 'w+') as f:\n",
    "    for values in df[\"Value\"]:\n",
    "        f.write(values + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cf18c",
   "metadata": {},
   "source": [
    "### Create model \n",
    "\n",
    "FastText possess a method to create and train a model automatically so let's use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a18d37d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  281132\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2805986 lr:  0.000000 avg.loss:  0.427186 ETA:   0h 0m 0s 67.4% words/sec/thread: 2791510 lr:  0.032629 avg.loss:  0.485156 ETA:   0h 0m 1s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(\"FastText_input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6de4d",
   "metadata": {},
   "source": [
    "### Save model\n",
    "\n",
    "The model was fast to compute but we can always save it as a file to keep it after shutting down kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65b6b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a6a2a",
   "metadata": {},
   "source": [
    "### Test model predictions\n",
    "\n",
    "Let's test some easy sentences to see if the model predict them correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad855236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.99994802]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I loved this movie !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1359d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.98021179]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I hated this movie !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b6885e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.86012012]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I really enjoyed how bad the actors played\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2be0d780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.97754908]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I did not enjoyed it as I should have\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb438911",
   "metadata": {},
   "source": [
    "The model seems to correct nicely most of the sentences but the last one is maybe too ambigous and is mispredicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7ab2d",
   "metadata": {},
   "source": [
    "### Test our accuracy\n",
    "\n",
    "A few sentences are great, but using a test dataset is better. Hopefully we got one so let's see the scores for predicting all 25000 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ea32b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_tests(x_test:list, y_test:list, model:fasttext.FastText._FastText) -> list:\n",
    "    \"\"\"\n",
    "    Calculate accuracy of predictions for a model\n",
    "    \n",
    "    Input:\n",
    "        x_test : list of sentences to predict\n",
    "        y_test : list of labels for x_test sentences\n",
    "        model : model to predict from\n",
    "        \n",
    "    Output:\n",
    "        List of predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    number_doc = len(x_test)\n",
    "    \n",
    "    for i in range(number_doc):\n",
    "        pred = model.predict(x_test[i])\n",
    "        \n",
    "        #\n",
    "        if pred[0][0] == '__label__negative':\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(1)\n",
    "                \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eb4c8533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 65 ms, total: 1.66 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = pred_tests(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5245197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6ccc91a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     12500\n",
      "           1       0.86      0.86      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2d42a",
   "metadata": {},
   "source": [
    "0.86 is better than logistic regression or naive bayesian implementation. We expected this because FastText is a great tool to create powerful models. Now, we need to see if we can improve the model to get better scores. \n",
    "\n",
    "First we can try to tune hyperparameters to see if FastText can offer a better model without modifying the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85de7d8",
   "metadata": {},
   "source": [
    "### Hyperparameters fitting\n",
    "\n",
    "To tune hyperparameters, we could do it by hand and try multiples values of learning rates, ngram-word and epoch. But FastText provide an all-in-one method to search for the better hyperparameters combinations for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443a911",
   "metadata": {},
   "source": [
    "#### Creating val and train dataset for hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fd071156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_FastText_file(value_column:list, file_name:str):\n",
    "    \"\"\"\n",
    "    Create a file with FastText Format from a DataFrame\n",
    "    \n",
    "    Input:\n",
    "        value_column : column of dataFrame containing formatted values\n",
    "        file_name : name of the file to create\n",
    "        \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file_name, \"w+\") as f:\n",
    "        for values in value_column:\n",
    "            f.write(values + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d030fa",
   "metadata": {},
   "source": [
    "FastText need a train and validation dataset to tune the hyperparameters according to the validation score. We makes one of 2000 samples, the 23000 remaining samples are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "af0736c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create val dataset as FastText input file\n",
    "list_to_FastText_file(df[:2000][\"Value\"],\"FastText_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0a77ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset as FastText input file\n",
    "list_to_FastText_file(df[2000:][\"Value\"], \"FastText_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52514982",
   "metadata": {},
   "source": [
    "#### Using FastText hyperparameters tuning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "56c96ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   12 Best score:  0.881500 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 5M words\n",
      "Number of words:  266902\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 4008359 lr:  0.000000 avg.loss:  0.060689 ETA:   0h 0m 0s  7.7% words/sec/thread: 3800757 lr:  0.089260 avg.loss:  0.387947 ETA:   0h 0m32s 64.3% words/sec/thread: 3974928 lr:  0.034480 avg.loss:  0.090183 ETA:   0h 0m11s\n"
     ]
    }
   ],
   "source": [
    "tuned_model = fasttext.train_supervised(input='FastText_train.txt', autotuneValidationFile='FastText_val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9e37a629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87     12500\n",
      "           1       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n",
      "CPU times: user 1.37 s, sys: 13.4 ms, total: 1.38 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred_tuned = pred_tests(x_test, y_test, tuned_model)\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3812d5",
   "metadata": {},
   "source": [
    "In 5min of hyperparameters autotune using a validation file, FastText found a best score of 0.88 which is better than our previous 0.86 score.\n",
    "\n",
    "In order to increase this score, we could let autotune execute longer but the best option is to apply treatment to the input data to remove stopword and use lemming/stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3589f",
   "metadata": {},
   "source": [
    "### Testing pretreatment methods\n",
    "\n",
    "Pretreating the data may be the most complicated part because of the large volume of samples in the dataset. Applying stopword removal, stemming and/or flemming can be very long to execute.\n",
    "\n",
    "First, let's create an all-in-one function to create and train a model so we just need to put pretreated samples as input to calculate the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "686115ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_predict(x_train:list, y_train:list ,x_test:list, y_test:list, file_name:str, autotune:bool=False, save:bool=True):\n",
    "    \"\"\"\n",
    "    Predict samples from x_test using FastText trained model with x_train samples.\n",
    "    \n",
    "    Input:\n",
    "        x_train: list of samples (train)\n",
    "        y_train: list of labels (train)\n",
    "        x_test: list of samples (test)\n",
    "        y_test: list of labels (test)\n",
    "        file_name: name of file created for FastText input formatting\n",
    "        autotune: specify if model need to autotune hyperparameters with a validation dataset\n",
    "        save: specify if test dataset needs to be save as file\n",
    "        \n",
    "    Output:\n",
    "        model and predictions of x_test\n",
    "    \"\"\"\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({'Value':x_train, 'Positive':y_train})\n",
    "    \n",
    "    # Format samples\n",
    "    df[\"Value\"] = df.apply(lambda x: add_label(x.Value, x.Positive), axis=1)\n",
    "    \n",
    "    # Shuffle data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # If autotune is True, create validation dataset from 20%\n",
    "    if autotune:\n",
    "        val_size = int(len(x_train) * 0.2)\n",
    "    \n",
    "        # Create file for validation set\n",
    "        list_to_FastText_file(df[:val_size][\"Value\"],file_name + \"_AutoTune_val.txt\")\n",
    "        \n",
    "        # Create file for train set\n",
    "        list_to_FastText_file(df[val_size:][\"Value\"], file_name + \"_AutoTune_train.txt\")\n",
    "        \n",
    "        \n",
    "        # Train model\n",
    "        print(\"Autotuning model\")\n",
    "        model = fasttext.train_supervised(input=file_name + \"_AutoTune_train.txt\",\\\n",
    "                                          autotuneValidationFile=file_name + \"_AutoTune_val.txt\")\n",
    "        \n",
    "    else:\n",
    "        # Create file for train\n",
    "        list_to_FastText_file(df[\"Value\"], file_name + \"_train.txt\")\n",
    "        \n",
    "        model = fasttext.train_supervised(input=file_name + \"_train.txt\")\n",
    "        \n",
    "    if save:\n",
    "        # Create test df\n",
    "        test_df = pd.DataFrame({'Value':x_test, 'Positive':y_test})\n",
    "\n",
    "        # Format tests samples\n",
    "        test_df[\"Value\"] = test_df.apply(lambda x: add_label(x.Value, x.Positive), axis=1)\n",
    "\n",
    "        # Create file for test\n",
    "        list_to_FastText_file(test_df[\"Value\"], file_name + \"_test.txt\")\n",
    "    \n",
    "    # Predict test\n",
    "    print(\"Predicting test samples\")\n",
    "    y_pred = pred_tests(x_test, y_test, model)\n",
    "    \n",
    "    # Print scores\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return (model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012eda71",
   "metadata": {},
   "source": [
    "#### Now we just need to add pretreatment and test a new training\n",
    "\n",
    "We can start by removing stopword to see if it affects the prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "57fb949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3eca68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FastText_score(preproc_func, file_name, remove_stopwords: bool = False, autotune: bool = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generate model and predictions from x_test with model trained on pretreated \n",
    "        x_train with preproc_func function and save as FastText input in file_name.\n",
    "    \n",
    "    Input:\n",
    "        preproc_func: preprocessing function.\n",
    "        file_name: name of file to save pretreated dataset.\n",
    "        remove_stopword: boolean to specify if stopwords needs to be removed (default: False).\n",
    "        \n",
    "    Output:\n",
    "        FastText model and predictions of x_test.\n",
    "    \"\"\"\n",
    "    # Apply pretreatment to x_train\n",
    "    print(\"Process x_train\")\n",
    "    with Pool() as p:\n",
    "        preproc_x_train = p.map(functools.partial(preproc_func, remove_stopwords=remove_stopwords), x_train)\n",
    "    \n",
    "    # Apply pretreatment to x_test\n",
    "    print(\"Process x_test\")\n",
    "    with Pool() as p:\n",
    "        preproc_x_test = p.map(functools.partial(preproc_func, remove_stopwords=remove_stopwords), x_test)\n",
    "        \n",
    "    return full_predict(preproc_x_train, y_train, preproc_x_test, y_test, file_name, autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64005800",
   "metadata": {},
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "88f57149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "cbc6d74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process x_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process x_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Read 2M words\n",
      "Number of words:  73049\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2342250 lr:  0.000000 avg.loss:  0.307644 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88     12500\n",
      "           1       0.87      0.88      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = get_FastText_score(pp.basic, \"No_StopWords\", remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f3121",
   "metadata": {},
   "source": [
    "#### Removing stopword + stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "37ebaf9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process x_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process x_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Read 3M words\n",
      "Number of words:  49050\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2737150 lr:  0.000000 avg.loss:  0.324717 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     12500\n",
      "           1       0.87      0.88      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemming_nostop = get_FastText_score(pp.stemming, \"Stemming_NoStop\", remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704f6be",
   "metadata": {},
   "source": [
    "#### Removing stopword + lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "91eba1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process x_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process x_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quentinlehelloco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Read 2M words\n",
      "Number of words:  60661\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2443393 lr:  0.000000 avg.loss:  0.315808 ETA:   0h 0m 0s 86.2% words/sec/thread: 2568275 lr:  0.013765 avg.loss:  0.333495 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     12500\n",
      "           1       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemming_nostop = get_FastText_score(pp.lemming, \"lemming_NoStop\", remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919bddc",
   "metadata": {},
   "source": [
    "It seems that only removing stopwords give the better results, let's try to tune hyperparameter for some models.\n",
    "\n",
    "\n",
    "To avoid more computation, we can now retrieve the pretreated data from the FastText input files with the right function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "29b65d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_from_FastText_file(file:str) -> (list, list):\n",
    "    \"\"\"\n",
    "    Load list from FastText input file\n",
    "    \n",
    "    Input:\n",
    "        file: name of the saved file\n",
    "        \n",
    "    Output:\n",
    "        samples and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = []\n",
    "    samples = []\n",
    "    \n",
    "    with open(file, \"r+\") as f:\n",
    "        lines = f.readlines()\n",
    "        length = len(lines)\n",
    "        \n",
    "        for i in range(length):\n",
    "            label = -1\n",
    "            \n",
    "            if '__label__positive ' in lines[i]:\n",
    "                label = 1\n",
    "            elif '__label__negative ' in lines[i]:\n",
    "                label = 0\n",
    "                \n",
    "            labels.append(label)\n",
    "            samples.append(lines[i].replace('__label__positive ', '').replace('__label__negative ', '').replace('\\n', ''))\n",
    "            \n",
    "    return samples, labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68075ad5",
   "metadata": {},
   "source": [
    "#### Autotune lemming + no stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7ca9722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretreated datasets from file to avoid more computation.\n",
    "lemming_x_train, lemming_y_train = load_list_from_FastText_file(\"lemming_NoStop_train.txt\")\n",
    "lemming_x_test, lemming_y_test = load_list_from_FastText_file(\"lemming_NoStop_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8ee3c027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   12 Best score:  0.870600 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 2M words\n",
      "Number of words:  54963\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1474224 lr:  0.000000 avg.loss:  0.347242 ETA:   0h 0m 0s 62.0% words/sec/thread: 1465431 lr:  0.004768 avg.loss:  0.441059 ETA:   0h 0m11s% words/sec/thread: 1466029 lr:  0.004378 avg.loss:  0.430805 ETA:   0h 0m10s 74.5% words/sec/thread: 1469048 lr:  0.003198 avg.loss:  0.402465 ETA:   0h 0m 7s 77.7% words/sec/thread: 1470609 lr:  0.002799 avg.loss:  0.394025 ETA:   0h 0m 6s 87.2% words/sec/thread: 1473732 lr:  0.001610 avg.loss:  0.371904 ETA:   0h 0m 3s 94.8% words/sec/thread: 1474570 lr:  0.000652 avg.loss:  0.356613 ETA:   0h 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87     12500\n",
      "           1       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemming_autotune = full_predict(lemming_x_train, lemming_y_train, \\\n",
    "                                lemming_x_test, lemming_y_test, \\\n",
    "                                \"lemming_NoStop\", autotune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549a02e",
   "metadata": {},
   "source": [
    "#### Autotune stopword removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e0af3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretreated datasets from file to avoid more computation.\n",
    "stopword_x_train, stopword_y_train = load_list_from_FastText_file(\"No_StopWords_train.txt\")\n",
    "stopword_x_test, stopword_y_test = load_list_from_FastText_file(\"No_StopWords_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0fa30a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   10 Best score:  0.876200 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 2M words\n",
      "Number of words:  66849\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2252626 lr:  0.000000 avg.loss:  0.332764 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     12500\n",
      "           1       0.87      0.88      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopword_autotune = full_predict(stopword_x_train, stopword_y_train, \\\n",
    "                                stopword_x_test, stopword_y_test, \\\n",
    "                                \"No_Stop\", autotune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ff54d",
   "metadata": {},
   "source": [
    "#### Autotune stemming + stopword removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7cbddd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretreated datasets from file to avoid more computation.\n",
    "stemming_x_train, stemming_y_train = load_list_from_FastText_file(\"Stemming_NoStop_train.txt\")\n",
    "stemming_x_test, stemming_y_test = load_list_from_FastText_file(\"Stemming_NoStop_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7832811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   10 Best score:  0.874000 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 2M words\n",
      "Number of words:  44584\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2595407 lr:  0.000000 avg.loss:  0.343354 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87     12500\n",
      "           1       0.88      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemming_autotune = full_predict(stemming_x_train, stemming_y_train, \\\n",
    "                                stemming_x_test, stemming_y_test, \\\n",
    "                                \"Stemming_NoStop\", autotune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b5ca6",
   "metadata": {},
   "source": [
    "### Find wrongly classified samples to analyze why it failed\n",
    "\n",
    "To understand why some samples were predicted incorrectly, we can take a look at some of it to see if we can detect a pattern that may be difficult for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f8ee2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_class(x_pred:list, y_pred:list, y_true:list, nb_samples=None) -> list:\n",
    "    \"\"\"\n",
    "    Find wrong classification of samples in x_pred based on y_pred prediction\n",
    "    \n",
    "    Input:\n",
    "        x_pred: list of samples\n",
    "        y_pred: list of predicted labels\n",
    "        y_true: list of true labels\n",
    "        nb_samples: max number of samples to extract (could be less if not enough in provided samples)\n",
    "    \"\"\"\n",
    "    wrongly_classified = []\n",
    "    count_sample = 0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if nb_samples is not None and count_sample == nb_samples:\n",
    "            break\n",
    "        \n",
    "        if y_pred[i] != y_true[i]:\n",
    "            wrongly_classified.append((x_pred[i], y_pred[i]))\n",
    "            count_sample += 1\n",
    "            \n",
    "    return wrongly_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5fd28832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Four things intrigued me as to this film - firstly, it stars Carly Pope (of \"Popular\" fame), who is always a pleasure to watch. Secdonly, it features brilliant New Zealand actress Rena Owen. Thirdly, it is filmed in association with the New Zealand Film Commission. Fourthly, a friend recommended it to me. However, I was utterly disappointed. The whole storyline is absurd and complicated, with very little resolution. Pope\\'s acting is fine, but Owen is unfortunately under-used. The other actors and actresses are all okay, but I am unfamiliar with them all. Aside from the nice riddles which are littered throughout the movie (and Pope and Owen), this film isn\\'t very good. So the moral of the story is...don\\'t watch it unless you really want to.',\n",
       " 1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_wrong = find_wrong_class(x_test, y_pred, y_test)\n",
    "few_wrong[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6ba6d13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The plot of this movie is as dumb as a bag of hair. Jimmy Smit plays a character that could have been upset by the ridiculousness of the story. He is evil and a wife beater. It's a character as far from his NYPD and LA Law roles as you could possibly get.<br /><br />If you've thought he had the looks and the acting chops to play the really bad boy role, her's your present.<br /><br />But!!!!!!!! Mary Louis Parker wears black miniskirts and little black minidresses throughout the movie.<br /><br />She has always had some of the greatest legs in the history of the movies. This makes the movie well worth it for this leg admirer.<br /><br />I'd buy the DVD for this reason only if it was available.\",\n",
       " 0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_wrong[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964d3ef",
   "metadata": {},
   "source": [
    "If we take those two samples that were mispredicted, we can see that they do not clearly state if they like or not the movie but implies it with complicated word and phrasing of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335f613",
   "metadata": {},
   "source": [
    "### Improve model\n",
    "\n",
    "Now that we have seen all that we could do with FastText, let's try to beat the baseline and target a prediction score of 0.90.\n",
    "\n",
    "To do so, we are going to use pretrained embedding by FastText and merge the into a classifier to finetune them for our particular sentiment prediction project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1d49311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Download model if not already exists\n",
    "fasttext.util.download_model('en', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6b6ba567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a3659848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_vector(sample:list):\n",
    "    \"\"\"\n",
    "    Get the average vector of all words in sample from pretrained FastText model\n",
    "    \n",
    "    Input:\n",
    "        sample: string containing words\n",
    "        \n",
    "    Output:\n",
    "        average vector for a sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize sample in single words\n",
    "    words = word_tokenize(sample)\n",
    "        \n",
    "    # Create list of vectors\n",
    "    vectors = []\n",
    "        \n",
    "    for w in words:\n",
    "        vectors.append(ft.get_word_vector(w))\n",
    "            \n",
    "    # Make average vector\n",
    "    vectors = np.asarray(vectors)\n",
    "    \n",
    "    average = np.average(vectors, axis=0)\n",
    "        \n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec458e",
   "metadata": {},
   "source": [
    "Now that we can get a average embedding of a sample, let's do it for all our dataset and use it as a classifier input dataset for our sentiment analysis prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "088ad6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 58.4 s, total: 2min 30s\n",
      "Wall time: 7min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get all dataset embedding average vectors\n",
    "embedding_x_train = []\n",
    "embedding_y_train = stopword_y_train\n",
    "\n",
    "for samples in stopword_x_train:\n",
    "    embedding_x_train.append(get_sample_vector(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0709a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 26.9 s, total: 1min 46s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get all test dataset embedding average vectors\n",
    "embedding_x_test = []\n",
    "embedding_y_test = stopword_y_test\n",
    "\n",
    "\n",
    "for samples in stopword_x_test:\n",
    "    embedding_x_test.append(get_sample_vector(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d66c4416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 300), (25000, 300))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_x_train = np.asarray(embedding_x_train)\n",
    "embedding_x_test = np.asarray(embedding_x_test)\n",
    "\n",
    "embedding_x_train.shape, embedding_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a10b6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure not to lose our data\n",
    "np.save(\"save_embedding_xtrain\", embedding_x_train)\n",
    "np.save(\"save_embedding_xtest\", embedding_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2736103",
   "metadata": {},
   "source": [
    "Now we can use a classic classifier to predict our sentiment from those vector samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "84d083c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "3c8b9eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline for SVM classifier\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(embedding_x_train, embedding_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "51822ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84056"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(embedding_x_test, embedding_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b1c9d",
   "metadata": {},
   "source": [
    "We get a accuracy that is worse than when using FastText directly.\n",
    "\n",
    "To improve this score we could search for another method to merge embedding from each words in a sample or use another type of model such as linear regression of random forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Console Py",
   "language": "python",
   "name": "console_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
